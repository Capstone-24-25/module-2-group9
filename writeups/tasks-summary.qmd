---
title: "Summary of exploratory tasks"
author: 'Leena Anqud, Rebecca Chang, Peter Xiong,'
date: today
---

### HTML scraping

**Does including header content improve predictions? Answer the question and provide quantitative evidence supporting your answer.**
Including header content does not seem to improve predictions. Our metrics of interest were sensitivity, specificity, accuracy, and ROC AUC, as our model is a binary classification model distinguishing between the presence or lack of presence of relevant content. The data that only included paragraph content has an accuracy of 0.835. Alternatively, the data that captured both paragraph and header content had an accuracy score of 0.779. This outcome may be expected, as including the header content introduces noise to our model. When we initially parsed the HTML content, the function transforms our text by removing unwanted elements like URLS, emails, punctuation, digits, and whitespace. In this project, the headers serve as a brief identification of the larger paragraph content listed, so it's reasonable to assume that the lack of meaningful content might weaken the predictive accuracy of a model that it is used to working with. Additionally, we used techniques of principal component analysis, retaining 70% of the variance, and logistic principal component regression to reduce dimensionality. These techniques are sensitive to noise, which could explain the reduction in model accuracy once the header content was included.
The paragraph and header content received a score of 0.795 for sensitivity, 0.762 for specificity, and 0.847 for ROC AUC. The paragraph content without headers received a score of .88 for sensitivity, 0.790 for specificity, and 0.878 for ROC AUC. Based on these results, our paragraph only data is better at identifying true positives, with a stronger ability to discern between our two classes as identified by the improved ROC AUC score. Interestingly, the paragraph and header content was slightly better at identifying true negatives. However, the paragraph only content has overall better predictive accuracy, so the marginal discrepancy in specificity score can be overlooked for the generally more favorable other metrics. The inclusion of headers ultimately reducing the predictive accuracy of our model potentially indicates that the content captured by the headers resulted in some sort of model overfitting, making it more difficult to identify relevant text and accurately classify the information.


### Bigrams

**Do bigrams capture additional information relevant to the classification of interest? Answer the question, briefly describe what analysis you conducted to arrive at your answer, and provide quantitative evidence supporting your answer.**
We began our analysis with a secondary tokenization of our cleaned data into bigrams, once again removing stopwords and punctuation. Similarly to Task 1, we compute the Term Frequency-Inverse Document Frequency (TF-IDF) score to quantify the relevance of the bigrams. We followed an 80/20 split of the data into training and testing sets, and applied both principal component analysis and singluar value decomposition to once again reduce dimensionality, fitting all of this on our logistic regression model. Once the model was fit, we calculated the same evaluation metrics as task 1. Our metrics were 0.324 for sensitivity, 0.809 for specificity, 0.550 for accuracy, and 0.531 for ROC AUC. Based on these results, bigrams do not capture additional information relevant to the classification of interest. Utilizing bigrams resulted in our model operating only slightly better than random guessing, as indicated by the ROC AUC score close to 0.5. However, this version of the model was best at classifying content as irrelevant, in comparison to the standard paragraph only content and paragraph and header content.

### Neural net

**Summarize the neural network model you trained:**
The trained neural network model had an input layer that corresponds to the number of features in the training set, two hidden layers with 128 units each, the sigmoid activation function to produce an output between 0 and 1 for binary classification, and an output layer containing the single unit that predicts the probability of relevant content. Our optimizer was Adam, optimizing the weights of the neural network while minimizing the loss function, ***binary_crossentropy***. Training and testing data were split 80/20 with 10 training epochs, to increase accuracy and decrease loss on the training set through each iteration. Looking at our plots for predictive accuracy, we observe our training sets performance was significantly better than our validation set. The training set improved across each epoch, stabilizing near 1 by epoch 4. However, our validation set, had marginal improvements in accuracy with each iteration. Additionally, loss increased for the validation set over each iteration, while it decreased for our training set. This might indicate potential overfitting on our training set, and we must find a way for the model to better generalize going forward.