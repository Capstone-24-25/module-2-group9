
## Script for Kuan-I Lu's work

### Preliminary task 1

```{r}
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>% # Include header to created augmented html text
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}
```

```{r}
# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean_head <- claims_raw %>%
  parse_data()

# export
save(claims_clean_head, file = '../data/claims-clean-head.RData')
```

  Visualize the dataset, claims_clean_head (2140), which is our starting point
  
```{r}
claims_clean_head %>% head()

claims_clean_head %>% ggplot(aes(x = bclass)) + geom_bar()
claims_clean_head %>% ggplot(aes(x = mclass)) + geom_bar()

claims_clean_head$text_clean %>% length()
```

```{r}
# Create task1_data for task 1
task1_data = claims_clean_head %>% 
  select(.id, bclass, text_clean)

task1_data %>% head()
```

  tokenize the texts
  
```{r}
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

stopwords_nopunct %>% length()

# Tokenize and construct the tf_idf term matrix
task1_data_tfidf_matrix = task1_data %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'token', 
                input = text_clean) %>%
  group_by(.id, bclass) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Remove stop words
task1_data_tfidf_matrix_clean <- task1_data_tfidf_matrix[, !names(task1_data_tfidf_matrix) %in% stopwords_nopunct]

```
  
  Perform PCN, and use the principle components(tokens) to fit a binary logistic regression

```{r}
# Refer to week 5 lecture and class activity
# path to activity files on repo
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))

```
  
```{r}
library(sparsesvd)
proj_out <- projection_fn(task1_data_tfidf_matrix_clean %>% select(-.id, -bclass), .prop = 0.7)
```


logistic principal component regression:

  use PCA to reduce variables, adn then use the reduces variables to do logistic regression.






### Preliminary task 3

benchmark: 
  accuracy: 0.835
  roc_auc: 0.878

```{r}
#install.packages("text")
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
library(text)
library(reticulate)
#install_keras(tensorflow = "2.16")
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5') %>% # Include header for better result, product of trial and error
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}


# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean <- claims_raw %>%
  parse_data()

claims_clean = claims_clean %>% 
  select(.id, mclass, bclass, text_clean)


write_csv(claims_clean, "claims_clean.csv")
```

We now have cleaned texts, which is claims_clean. This will be our starting point


```{r}
# Tokenize and construct the tf_idf term matrix
claims_clean_tfidf = claims_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'token', 
                input = text_clean) %>%
  group_by(.id, bclass, mclass) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass, mclass), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Load a list of stop words
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

# Remove stop words
claims_clean_tfidf <- claims_clean_tfidf[, !names(claims_clean_tfidf) %in% stopwords_nopunct]

```

```{r}
# partition
set.seed(111524)
partitions <- claims_clean_tfidf %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix

y_train = training(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% # use as.matrix for multiclass
  as.numeric() - 1

x_test = testing(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix()

y_test = testing(partitions) %>% 
  pull(mclass, bclass) %>% 
  as.matrix()
  
```


```{r}
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(128) %>%
  layer_dense(128) %>% 
  layer_dense(1) %>%
  layer_activation(activation = 'sigmoid')

summary(model)
```

```{r}
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )
```

```{r}
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 10,
      validation_split = 0.2)
```

```{r}
plot(history)
```

The best accuracy we reached with our testing data is 0.8571, and the best roc_auc we get with out testing dataset is 0.907

### Primary task 1

benchmark: 
  accuracy: 0.835
  roc_auc: 0.878

#### Approach 4: modify preprocessing and then ANN

Many of the approaches doesn't yield ideal results, falling short from the benchmarked 0.835

Approach differently from refining preprocessing, lets include principle component of unigrams and bigrams

```{r}
# Load necessary packages
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(tidymodels)
library(keras)
library(tensorflow)
library(text)
library(reticulate)
library(sparsesvd)

# If model doesn't work, delete virtualenvs in documents and run this to load the right version of tf
#install_keras(tensorflow = "2.16")
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5') %>% # Include header for better result, product of trial and error
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}
```


```{r}
# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean <- claims_raw %>%
  parse_data()

claims_clean = claims_clean %>% 
  select(.id, mclass, bclass, text_clean)

```

##### binary class

```{r}
# Load and define a list of stop words
data("stop_words")
stop_words = stop_words %>% 
  filter(lexicon == "snowball") %>% 
  mutate(word = word %>% str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'), collapse = '|'), '') %>% 
           str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>% tolower()) %>% 
  distinct(word)

# Tokenize and construct the tf_idf term matrix of unigrams
claims_unigram_tfidf = claims_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'unigram', 
                input = text_clean) %>%   #### tokenize unigram
  anti_join(stop_words, by = c("unigram" = "word")) %>% #### remove stopwords
  group_by(.id, bclass) %>% #### change group for multiclass
  count(unigram) %>%
  bind_tf_idf(term = unigram, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass), #### change id_cols for multiclass
              names_from = unigram, 
              values_from = tf_idf,
              values_fill = 0) %>%  #### create tf_idf matrix
  ungroup() 

# Tokenize and construct the tf_idf term matrix of bigrams
claims_bigram_tfidf = claims_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'bigram', 
                input = text_clean,
                token = "ngrams", n = 2) %>%   #### tokenize bigram
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  unite(bigram, word1, word2, sep = " ") %>% #### remove bigrams containing stopwords
  group_by(.id, bclass) %>% #### change group for multiclass
  count(bigram) %>%
  bind_tf_idf(term = bigram, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass), #### change id_cols for multiclass
              names_from = bigram, 
              values_from = tf_idf,
              values_fill = 0) %>%  #### create tf_idf matrix
  ungroup() 

# combine the two datasets
claims_tfidf = claims_unigram_tfidf %>% right_join(claims_bigram_tfidf, by = c(".id" = ".id", "bclass" = "bclass"))
```


```{r}
# Split the data to training and testing with 0.8/0.2, stratified on 
set.seed(111524)
bclass_partition = initial_split(claims_tfidf, prop = 0.8, strata = bclass)

# Define training and testing set. Remove columns with zero variance
bclass_train = training(bclass_partition)
bclass_train_x = bclass_train %>% select(-.id, -bclass) %>% as.matrix() 
bclass_train_y = bclass_train %>% pull(bclass) %>% as.numeric() - 1

bclass_test = testing(bclass_partition)
bclass_test_x = bclass_test %>% select(-.id, -bclass) %>% as.matrix()
bclass_test_y = bclass_test %>% pull(bclass)
  
```


```{r}
# build the NN model
# Specify model type
library(keras3)

# Model building, 2 hidden layers
model <- keras_model_sequential(input_shape = ncol(bclass_train_x)) %>%
  layer_dense(128) %>%
  layer_dense(128) %>% 
  layer_dense(1) %>%  # output layer, use softmax for multiclass
  layer_activation(activation = 'sigmoid')

summary(model)
```


```{r}
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )


# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history = model %>%
  fit(x = bclass_train_x,
      y = bclass_train_y,
      epochs = 8,
      validation_split = 0.2)

```















#### Approach 5: going back to FFNN, task 3 result good enough

benchmark: accuracy: 0.835 roc_auc: 0.878

Attempt to use NN to reach similar result

```{r}
#install.packages("text")
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
library(text)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(reticulate)
library(purrr)

#install_keras(tensorflow = "2.16")
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5') %>% # Include header for better result, product of trial and error
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}


# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean <- claims_raw %>%
  parse_data()

claims_clean = claims_clean %>% 
  select(.id, mclass, bclass, text_clean)

```

We now have cleaned texts, which is claims_clean. This will be our starting point

```{r}
# Tokenize and construct the tf_idf term matrix
claims_clean_tfidf = claims_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'token', 
                input = text_clean) %>%
  group_by(.id, bclass, mclass) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass, mclass), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Load a list of stop words
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

# Remove stop words
claims_clean_tfidf <- claims_clean_tfidf[, !names(claims_clean_tfidf) %in% stopwords_nopunct]

```

##### binary class

```{r}
# partition
set.seed(111524)
partitions <- claims_clean_tfidf %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix

y_train = training(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% 
  as.numeric() - 1
binary_levels = levels(training(partitions) %>% pull(bclass) %>% factor())

x_test = testing(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix()

y_test = testing(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% 
  as.numeric() - 1
  
```

```{r}
set.seed(111524)
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(256) %>%
  layer_dense(256) %>% 
  layer_dense(1) %>%
  layer_activation(activation = 'sigmoid')

summary(model)
```

```{r}
set.seed(111524)
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )
```

```{r}
set.seed(111524)
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 6)
```

###### Evaluate prediction


```{r}
# Evaluate accuracy on testing set
test_results <- model %>% evaluate(x_test, y_test)
```

  Our testing accuracy in theh etstin gdata is 0.82, which is the best we can obtain.

###### Make predictions on testing dataset

```{r}
# Import the claims_test dataset
load('../data/claims-test.RData')

# Clean clains_test
claims_test_clean <- claims_test %>%
  parse_data()
claims_test_clean = claims_test_clean %>% 
  select(.id, text_clean)

# Notice there are 19 observations with an empty string, we cannot predict any value from that
# This issue may be from the parsing but we will investigate later
claims_test_clean %>% filter(text_clean == "")

# Tokenize and construct the tf_idf term matrix
claims_test_tfidf = claims_test_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>% 
  unnest_tokens(output = 'token', 
                input = text_clean) %>% 
  group_by(.id) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Load a list of stop words
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

# Remove stop words
claims_test_tfidf <- claims_test_tfidf[, !names(claims_test_tfidf) %in% stopwords_nopunct]

# Align the dimention of claims_test_tfidf claims_clean_tfidf
claims_test_tfidf = claims_test_tfidf %>%
  bind_cols(as.data.frame(matrix(0, nrow = nrow(claims_test_tfidf), 
                                 ncol = length(setdiff(names(claims_clean_tfidf), names(claims_test_tfidf))), 
                                 dimnames = list(NULL, setdiff(names(claims_clean_tfidf), names(claims_test_tfidf)))))) %>%
  select(all_of(names(claims_clean_tfidf))) %>% 
  select(-bclass, -mclass)

```

```{r}
# Make predictions on the testing data
claims_test_tfidf_id = claims_test_tfidf %>% select(.id)
claims_test_tfidf_predictor = claims_test_tfidf %>% select(-.id) %>% as.matrix() ###### use this directly for multiclass 

binary_prediction = model %>% predict(claims_test_tfidf_predictor)

pred_df = claims_test_tfidf_id %>% 
  mutate(bclass.pred = binary_prediction) %>% 
  mutate(bclass.pred = ifelse(bclass.pred > 0.5, 2, 1)) %>% 
  mutate(bclass.pred = binary_levels[bclass.pred])
```


##### multi class

```{r}
# partition
set.seed(111524)
partitions <- claims_clean_tfidf %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix

y_train = training(partitions) %>% 
  pull(mclass) %>% 
  factor() %>% 
  as.numeric() - 1
multiclass_levels = levels(training(partitions) %>% pull(mclass) %>% factor())

x_test = testing(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix()

y_test = testing(partitions) %>% 
  pull(mclass, bclass) %>% 
  factor() %>% # use as.matrix for multiclass
  as.numeric() - 1
  
```

```{r}
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(256) %>%
  layer_dense(256) %>% 
  layer_dense(5) %>%
  layer_activation(activation = 'softmax')

summary(model)
```

```{r}
# compile the model
model %>%
  compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('sparse_categorical_accuracy')
  )
```

```{r}
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 6)
```

###### Evaluate prediction


```{r}
# Evaluate accuracy on testing set
test_results <- model %>% evaluate(x_test, y_test)
```

  We reached 80% sparse categorical accuracy on the testing data

###### Make predictions on testing dataset

```{r}
multiclass_prediction = model %>% predict(claims_test_tfidf_predictor)

pred_df = pred_df %>% 
  cbind(data.frame(multiclass_prediction)) %>% 
  rowwise() %>% 
  mutate(mclass.pred = which.max(c_across(X1:X5))) %>%
  ungroup() %>% 
  select(.id, bclass.pred, mclass.pred) %>% 
  mutate(mclass.pred = multiclass_levels[mclass.pred])

# Save the results
save(pred_df, file = "../results/preds-group[N].RData")
```



























 
#### Approach 1: DistilBERT with R

```{r}
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(rsample)
library(keras3)
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5') %>% # Include header for better result, product of trial and error
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}


# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean <- claims_raw %>%
  parse_data()

claims_clean = claims_clean %>% 
  select(.id, mclass, bclass, text_clean)
```

```{r}
# partition
set.seed(111524)
partitions <- claims_clean %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(text_clean) 
x_train = x_train$text_clean

y_train = training(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% # use as.matrix for multiclass
  as.numeric() - 1

x_test = testing(partitions) %>% 
  select(text_clean)
x_test = x_test$text_clean

y_test = testing(partitions) %>% 
  pull(mclass, bclass) %>% 
  as.matrix()

```

```{r}
library(text)
# Pre-trained embeddings using BERT
text_embeddings = text::textEmbed(texts = x_train, 
                                  model = "bert-base-uncased",
                                  aggregation_from_tokens_to_texts = "mean",
                                  aggregation_from_tokens_to_word_types = NULL)
text_embeddings = text_embeddings$texts$texts


embeddings <- text::textEmbed(
  texts = c("This is a test sentence.", "Another example."),
  model = "bert-base-uncased"
)
embeddings$texts$texts


model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(ncol(text_embeddings))) %>%
  layer_dense(units = 1, activation = "sigmoid") # Binary Classification

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = list('binary_accuracy',  metric_auc())
)

# Train the model
history <- model %>% fit(
  x = text_embeddings, 
  y = y_train,
  epochs = 8,
  batch_size = 32,
  validation_split = 0.2
)


model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(ncol(text_embeddings))) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

model %>% fit(
  x = text_embeddings, y = labels,
  epochs = 10, batch_size = 32, validation_split = 0.2
)
reticulate::py_install("huggingface_hub", pip = TRUE)

```

```{r}
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 32, input_shape = c(10, 1)) %>%  # RNN with 32 units
  layer_dense(units = 1, activation = "sigmoid")           # Output layer for binary classification

summary(model)
```

```{r}
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )
```

```{r}
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 5,
      validation_split = 0.2)
```

```{r}
plot(history)
```

 
#### Approach 2: SVM

```{r}
library(e1071)  # For SVM
library(tm)     # For text preprocessing
library(caret)  # For model evaluation and training

```

 
```{r}
# Preprocess the text
corpus = claims_clean$text_clean
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
#  Convert the DTM to a data frame
dtm_df <- as.data.frame(as.matrix(dtm))

claims_clean %>% head()
```

```{r}
# Train test split
set.seed(1115)
train_indices <- createDataPartition(claims_clean$bclass, p = 0.8, list = FALSE)

x_train <- dtm_df[train_indices, ]  # Features for training
y_train <- claims_clean$bclass[train_indices]  # Labels for training
x_test <- dtm_df[-train_indices, ]  # Features for testing
y_test <- claims_clean$bclass[-train_indices]  # Labels for testing

```
 
 
```{r}
# Train the SVM model
svm_model <- svm(
  x = x_train,
  y = as.factor(y_train),
  kernel = "linear",  # Common for text data
  cost = 1,           # Regularization parameter
  scale = FALSE       # Disable feature scaling (already done in DTM)
)

```

```{r}
# Define the training control
control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the model with hyperparameter tuning
svm_tuned <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "svmLinear",
  trControl = control,
  tuneGrid = expand.grid(C = c(0.1, 1, 10))  # Test different cost values
)

# Print the best model
print(svm_tuned)

```


 
 
 
 
 
 
 
 
 
 
#### Approach 3: DistilBERT with Python
  
```{r}
library(reticulate)
```

```{python}
# Load the packages
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from transformers import BertModel
from transformers import DistilBertTokenizer
from transformers import TFDistilBertModel
from sklearn.model_selection import train_test_split

AUTOTUNE = tf.data.experimental.AUTOTUNE

```


```{r}
binary_data = claims_clean %>% select(bclass, text_clean) %>% mutate(bclass = ifelse(bclass == "Relevant claim content", 1, 0))
```

```{python}
# Load the data
data = r.binary_data
```

```{python}
# Tokenize the data
# Import the pretrained DistilBertTokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Set max length
max_length = 800

# Split the dataset into traning and testing data
################################# Change for multiclass 
train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=111524, stratify=data["bclass"])

# Seperate both train_data and text_data into data(text) and labels
train_data = train_dataset["text_clean"]
train_data = np.array(train_data)
train_labels = np.array(train_dataset["bclass"]).astype(int)

test_data = test_dataset["text_clean"]
test_data = np.array(test_data)
test_labels = np.array(test_dataset["bclass"]).astype(int)

# Tokenize the train data
train_tokenized = tokenizer(train_data, truncation=True, padding=True, max_length=max_length, return_tensors='tf')
print(type(train_data))
print(train_data[:5])
# Tokenize the test data
test_tokenized = tokenizer(test_data, truncation=True, padding=True, max_length=max_length, return_tensors='tf')

```


```{python}
# Initialize the TFDistilBertModel model with pre-trained weights
model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', name="DistilBER_Pretrained")

model.summary()
```

```{python}
# define the batch size, epochs, and learning rates
batch_size = 8
epochs = 5
learning_rate = 2e-05
```

```{python}
# Alter layers trainability for optimal training results and computational burden
# Unfreeze the last layer of the model
model.distilbert.transformer.layer[-1].trainable = True

# Freeze the rest of the layers
for layer in model.distilbert.transformer.layer[:-1]:
    layer.trainable = False
```

















