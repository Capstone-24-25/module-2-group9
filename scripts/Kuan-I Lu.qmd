
## Script for Kuan-I Lu's work

### Preliminary task 1

```{r}
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>% # Include header to created augmented html text
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}
```

```{r}
# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean_head <- claims_raw %>%
  parse_data()

# export
save(claims_clean_head, file = '../data/claims-clean-head.RData')
```

  Visualize the dataset, claims_clean_head (2140), which is our starting point
  
```{r}
claims_clean_head %>% head()

claims_clean_head %>% ggplot(aes(x = bclass)) + geom_bar()
claims_clean_head %>% ggplot(aes(x = mclass)) + geom_bar()

claims_clean_head$text_clean %>% length()
```

```{r}
# Create task1_data for task 1
task1_data = claims_clean_head %>% 
  select(.id, bclass, text_clean)

task1_data %>% head()
```

  tokenize the texts
  
```{r}
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

stopwords_nopunct %>% length()

# Tokenize and construct the tf_idf term matrix
task1_data_tfidf_matrix = task1_data %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'token', 
                input = text_clean) %>%
  group_by(.id, bclass) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Remove stop words
task1_data_tfidf_matrix_clean <- task1_data_tfidf_matrix[, !names(task1_data_tfidf_matrix) %in% stopwords_nopunct]

```
  
  Perform PCN, and use the principle components(tokens) to fit a binary logistic regression

```{r}
# Refer to week 5 lecture and class activity
# path to activity files on repo
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'
# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))

```
  
```{r}
library(sparsesvd)
proj_out <- projection_fn(task1_data_tfidf_matrix_clean %>% select(-.id, -bclass), .prop = 0.7)
```


logistic principal component regression:

  use PCA to reduce variables, adn then use the reduces variables to do logistic regression.






### Preliminary task 3

benchmark: 
  accuracy: 0.835
  roc_auc: 0.878

Maybe later:
  We attempt to use DistilBERT to yield better result

```{r}
#install.packages("text")
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
library(text)
install_keras(tensorflow = "2.16")
```

```{r}
# predefined functions
# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5') %>% # Include header for better result, product of trial and error
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}


# Load and parse the data
# load raw data
load('../data/claims-raw.RData')

# preprocess (will take a minute or two)
claims_clean <- claims_raw %>%
  parse_data()

claims_clean = claims_clean %>% 
  select(.id, mclass, bclass, text_clean)

```

We now have cleaned texts, which is claims_clean. This will be our starting point


```{r}
# Tokenize and construct the tf_idf term matrix
claims_clean_tfidf = claims_clean %>%
  mutate(text_clean = str_trim(text_clean)) %>%
  filter(str_length(text_clean) > 5) %>%
  unnest_tokens(output = 'token', 
                input = text_clean) %>%
  group_by(.id, bclass, mclass) %>%
  count(token) %>%
  bind_tf_idf(term = token, 
              document = .id, 
              n = n) %>%
  pivot_wider(id_cols = c(.id, bclass, mclass), 
              names_from = token, 
              values_from = tf_idf,
              values_fill = 0) %>%
  ungroup() 

# Load a list of stop words
stopwords_nopunct <- stopwords(language = 'en', source = 'snowball') %>% 
  str_remove_all('[[:punct:]]')

# Remove stop words
claims_clean_tfidf <- claims_clean_tfidf[, !names(claims_clean_tfidf) %in% stopwords_nopunct]

```

```{r}
# partition
set.seed(111524)
partitions <- claims_clean_tfidf %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix

y_train = training(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% # use as.matrix for multiclass
  as.numeric() - 1

x_test = testing(partitions) %>% 
  select(-mclass, -bclass, -.id) %>% 
  as.matrix()

y_test = testing(partitions) %>% 
  pull(mclass, bclass) %>% 
  as.matrix()
  
```


```{r}
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(128) %>%
  layer_dense(128) %>% 
  layer_dense(1) %>%
  layer_activation(activation = 'sigmoid')

summary(model)
```

```{r}
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )
```

```{r}
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 10,
      validation_split = 0.2)
```

```{r}
plot(history)
```

The best accuracy we reached with our testing data is 0.8571, and the best roc_auc we get with out testing dataset is 0.907

### Primary task 1
 
#### Approach 1: RNN (2 hidden layers)

```{r}
library(keras3)
```


```{r}
claims_clean = claims_clean %>% mutate(bclass = ifelse(bclass == "Relevant claim content", 1, 0))

# Tokenize and pad the texts
texts = claims_clean$text_clean
labels_bin = claims_clean$bclass
labels_mult = claims_clean$mclass

claims_clean %>% head()

```

```{r}
# partition
set.seed(111524)
partitions <- claims_clean %>%
  initial_split(prop = 0.8)

x_train = training(partitions) %>% 
  select(text_clean) %>% 
  as.vector()

y_train = training(partitions) %>% 
  pull(bclass) %>% 
  factor() %>% # use as.matrix for multiclass
  as.numeric() - 1

x_test = testing(partitions) %>% 
  select(text_clean) %>% 
  as.vector()

y_test = testing(partitions) %>% 
  pull(mclass, bclass) %>% 
  as.matrix()
  
```

```{r}
library(text)
# Pre-trained embeddings using BERT
text_embeddings <- text::textEmbed(texts = x_train, model = "bert-base-uncased")

embeddings <- text::textEmbed(
  texts = c("This is a test sentence.", "Another example."),
  model = "bert-base-uncased"
)

model <- keras_model_sequential() %>%
  layer_gru(units = 64, input_shape = c(1000, 1)) %>%
  layer_dense(units = 1, activation = "sigmoid")  # Binary classification

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = list('binary_accuracy',  metric_auc())
)

# Train the model
history <- model %>% fit(
  x = x_train, 
  y = y_train,
  epochs = 8,
  batch_size = 128,
  validation_split = 0.2
)











model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(ncol(text_embeddings))) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

model %>% fit(
  x = text_embeddings, y = labels,
  epochs = 10, batch_size = 32, validation_split = 0.2
)
reticulate::py_install("huggingface_hub", pip = TRUE)

```

```{r}
# Specify model type
library(keras3)

# Model building
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 32, input_shape = c(10, 1)) %>%  # RNN with 32 units
  layer_dense(units = 1, activation = "sigmoid")           # Output layer for binary classification

summary(model)
```

```{r}
# compile the model
model %>%
  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = list('binary_accuracy',  metric_auc())
  )
```

```{r}
# Fit the model, with 10 epoch, and cross validation with validation split of 0.2
history <- model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 5,
      validation_split = 0.2)
```

```{r}
plot(history)
```

 
#### Approach 2: SVM

```{r}
library(e1071)  # For SVM
library(tm)     # For text preprocessing
library(caret)  # For model evaluation and training

```

 
```{r}
# Preprocess the text
corpus = claims_clean$text_clean
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
#  Convert the DTM to a data frame
dtm_df <- as.data.frame(as.matrix(dtm))

claims_clean %>% head()
```

```{r}
# Train test split
set.seed(1115)
train_indices <- createDataPartition(claims_clean$bclass, p = 0.8, list = FALSE)

x_train <- dtm_df[train_indices, ]  # Features for training
y_train <- claims_clean$bclass[train_indices]  # Labels for training
x_test <- dtm_df[-train_indices, ]  # Features for testing
y_test <- claims_clean$bclass[-train_indices]  # Labels for testing

```
 
 
```{r}
# Train the SVM model
svm_model <- svm(
  x = x_train,
  y = as.factor(y_train),
  kernel = "linear",  # Common for text data
  cost = 1,           # Regularization parameter
  scale = FALSE       # Disable feature scaling (already done in DTM)
)

```

```{r}
# Define the training control
control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the model with hyperparameter tuning
svm_tuned <- train(
  x = x_train,
  y = as.factor(y_train),
  method = "svmLinear",
  trControl = control,
  tuneGrid = expand.grid(C = c(0.1, 1, 10))  # Test different cost values
)

# Print the best model
print(svm_tuned)

```


 
 
 
 
 
 
 
 
 
 
#### Approach 3: DistilBERT
  
```{r}
library(reticulate)
```

```{python}
# Load the packages
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from transformers import BertModel
from transformers import DistilBertTokenizer
from transformers import TFDistilBertModel
from sklearn.model_selection import train_test_split

AUTOTUNE = tf.data.experimental.AUTOTUNE

```

##### Binary classification

```{r}
binary_data = claims_clean %>% select(bclass, text_clean) %>% mutate(bclass = ifelse(bclass == "Relevant claim content", 1, 0))
```

```{python}
# Load the data
data = r.binary_data
```

```{python}
# Tokenize the data
# Import the pretrained DistilBertTokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Set max length
max_length = 800

# Split the dataset into traning and testing data
################################# Change for multiclass 
train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=111524, stratify=data["bclass"])

# Seperate both train_data and text_data into data(text) and labels
train_data = train_dataset["text_clean"]
train_data = np.array(train_data)
train_labels = np.array(train_dataset["bclass"]).astype(int)

test_data = test_dataset["text_clean"]
test_data = np.array(test_data)
test_labels = np.array(test_dataset["bclass"]).astype(int)

# Tokenize the train data
train_tokenized = tokenizer(train_data, truncation=True, padding=True, max_length=max_length, return_tensors='tf')
print(type(train_data))
print(train_data[:5])
# Tokenize the test data
test_tokenized = tokenizer(test_data, truncation=True, padding=True, max_length=max_length, return_tensors='tf')

```


```{python}
# Initialize the TFDistilBertModel model with pre-trained weights
model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', name="DistilBER_Pretrained")

model.summary()
```

```{python}
# define the batch size, epochs, and learning rates
batch_size = 8
epochs = 5
learning_rate = 2e-05
```

```{python}
# Alter layers trainability for optimal training results and computational burden
# Unfreeze the last layer of the model
model.distilbert.transformer.layer[-1].trainable = True

# Freeze the rest of the layers
for layer in model.distilbert.transformer.layer[:-1]:
    layer.trainable = False
```

















